{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kinsmMplkRc2",
        "outputId": "ff6192c9-9b53-4e33-b7c6-1a66229e0a10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.171.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FIRECRAWL_API_KEY = \"API-KEY\"  # Replace with your key\n",
        "\n",
        "session = requests.Session()  # reuse connection for speed\n",
        "\n",
        "def scrape_static(url, timeout=10):\n",
        "    \"\"\"\n",
        "    Static scrape: HTTP GET + BeautifulSoup parse.\n",
        "    Returns (title, text snippet, soup).\n",
        "    Raises RuntimeError on failure or insufficient content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = session.get(url, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"No title found\"\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        if len(text) < 200:\n",
        "            raise RuntimeError(\"Insufficient static content extracted.\")\n",
        "        return title, text[:1500], soup\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Static scrape failed: {e}\")\n",
        "\n",
        "def scrape_dynamic(url, timeout=20):\n",
        "    \"\"\"\n",
        "    Dynamic scrape using Firecrawl API for JS-rendered content.\n",
        "    Returns (title, text snippet, soup).\n",
        "    Raises RuntimeError on failure.\n",
        "    \"\"\"\n",
        "    endpoint = \"https://api.firecrawl.dev/v1/scrape\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {FIRECRAWL_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\"url\": url, \"dynamic\": True, \"extractorOptions\": {\"type\": \"html\"}}\n",
        "\n",
        "    try:\n",
        "        resp = session.post(endpoint, headers=headers, json=payload, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        html = resp.json().get(\"html\")\n",
        "        if not html:\n",
        "            raise RuntimeError(\"No HTML content returned by Firecrawl.\")\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        title = soup.title.string.strip() if soup.title else \"No title found\"\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return title, text[:1500], soup\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Firecrawl scraping failed: {e}\")\n",
        "\n",
        "def extract_headlines(soup):\n",
        "    \"\"\"\n",
        "    Extract and format all h1, h2, h3 headlines.\n",
        "    \"\"\"\n",
        "    headers = soup.find_all(['h1', 'h2', 'h3'])\n",
        "    if not headers:\n",
        "        return \"No headlines found.\"\n",
        "    return \"\\n\".join(f\"üîπ {h.get_text(strip=True)}\" for h in headers)\n",
        "\n",
        "def extract_links(soup, limit=50):\n",
        "    \"\"\"\n",
        "    Extract unique hrefs from anchors, up to limit.\n",
        "    \"\"\"\n",
        "    links = {link['href'].strip() for link in soup.find_all('a', href=True) if link['href'].strip()}\n",
        "    if not links:\n",
        "        return \"No links found.\"\n",
        "    return \"\\n\".join(f\"üîó {link}\" for link in list(links)[:limit])\n",
        "\n",
        "def extract_images(soup, limit=50):\n",
        "    \"\"\"\n",
        "    Extract unique image src URLs, up to limit.\n",
        "    \"\"\"\n",
        "    images = {img['src'].strip() for img in soup.find_all('img', src=True) if img['src'].strip()}\n",
        "    if not images:\n",
        "        return \"No images found.\"\n",
        "    return \"\\n\".join(f\"üñºÔ∏è {img}\" for img in list(images)[:limit])\n",
        "\n",
        "def extract_paragraphs(soup, limit=10):\n",
        "    \"\"\"\n",
        "    Extract text from up to limit paragraphs.\n",
        "    \"\"\"\n",
        "    paragraphs = soup.find_all('p')\n",
        "    if not paragraphs:\n",
        "        return \"No paragraphs found.\"\n",
        "    return \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs[:limit])\n",
        "\n",
        "def is_internal_link(base_url, link):\n",
        "    \"\"\"\n",
        "    Check if a link is internal (same domain as base_url).\n",
        "    \"\"\"\n",
        "    base_netloc = urlparse(base_url).netloc\n",
        "    link_netloc = urlparse(urljoin(base_url, link)).netloc\n",
        "    return base_netloc == link_netloc\n",
        "\n",
        "def extract_internal_links(soup, base_url):\n",
        "    \"\"\"\n",
        "    Extract unique internal links from soup.\n",
        "    \"\"\"\n",
        "    links = set()\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href'].strip()\n",
        "        full_url = urljoin(base_url, href).split('#')[0]  # strip fragment\n",
        "        if is_internal_link(base_url, full_url):\n",
        "            links.add(full_url)\n",
        "    return list(links)\n",
        "\n",
        "def get_element_path(element):\n",
        "    \"\"\"\n",
        "    Generate CSS-like path with sibling indices for an element.\n",
        "    \"\"\"\n",
        "    path = []\n",
        "    while element and element.name != '[document]':\n",
        "        sibling_index = 1\n",
        "        sibling = element.previous_sibling\n",
        "        while sibling:\n",
        "            if getattr(sibling, 'name', None) == element.name:\n",
        "                sibling_index += 1\n",
        "            sibling = sibling.previous_sibling\n",
        "        tag = f\"{element.name}[{sibling_index}]\" if sibling_index > 1 else element.name\n",
        "        path.append(tag)\n",
        "        element = element.parent\n",
        "    return \" > \".join(reversed(path))\n",
        "\n",
        "def search_keyword_in_soup(soup, keyword):\n",
        "    \"\"\"\n",
        "    Search keyword in <p> tags of soup.\n",
        "    Returns list of (element_path, snippet) or None.\n",
        "    \"\"\"\n",
        "    keyword_lower = keyword.lower()\n",
        "    results = []\n",
        "    for p in soup.find_all('p'):\n",
        "        text = p.get_text(strip=True)\n",
        "        text_lower = text.lower()\n",
        "        if keyword_lower in text_lower:\n",
        "            idx = text_lower.index(keyword_lower)\n",
        "            start = max(idx - 40, 0)\n",
        "            end = min(idx + 110, len(text))\n",
        "            snippet = text[start:end]\n",
        "            path = get_element_path(p)\n",
        "            results.append((path, snippet))\n",
        "    return results if results else None\n",
        "\n",
        "def search_keyword_on_page(url, keyword, timeout=10):\n",
        "    \"\"\"\n",
        "    Static scrape keyword search fallback.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = session.get(url, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "        return search_keyword_in_soup(soup, keyword)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    url = input(\"üîó Enter the website URL to scrape: \").strip()\n",
        "    menu = (\n",
        "        \"üéØ What do you want to extract? Choose one:\\n\"\n",
        "        \"    1: headlines\\n\"\n",
        "        \"    2: title\\n\"\n",
        "        \"    3: text (full page text)\\n\"\n",
        "        \"    4: links\\n\"\n",
        "        \"    5: images\\n\"\n",
        "        \"    6: content previews\\n\"\n",
        "        \"    7: search keyword in paragraphs\\n\"\n",
        "        \"Your choice (1-7): \"\n",
        "    )\n",
        "    choice = input(menu).strip()\n",
        "\n",
        "    choice_map = {\n",
        "        '1': 'headlines',\n",
        "        '2': 'title',\n",
        "        '3': 'text',\n",
        "        '4': 'links',\n",
        "        '5': 'images',\n",
        "        '6': 'paragraphs',\n",
        "        '7': 'search'\n",
        "    }\n",
        "\n",
        "    target_info = choice_map.get(choice)\n",
        "    if not target_info:\n",
        "        print(\" Invalid choice. Exiting.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        print(\"\\n Attempting dynamic scrape with Firecrawl API...\")\n",
        "        title, text, soup = scrape_dynamic(url)\n",
        "    except RuntimeError as e:\n",
        "        print(f\" Firecrawl dynamic scrape failed: {e}\")\n",
        "        print(\" Falling back to static scrape...\")\n",
        "        try:\n",
        "            title, text, soup = scrape_static(url)\n",
        "        except RuntimeError as ex:\n",
        "            print(f\" Both dynamic and static scraping failed: {ex}\")\n",
        "            return\n",
        "\n",
        "    if target_info == 'search':\n",
        "        keyword = input(\" Enter the keyword to search for in paragraphs: \").strip()\n",
        "        print(f\"\\n Searching keyword '{keyword}' across multiple pages (limit 10 pages)...\\n\")\n",
        "\n",
        "        internal_links = extract_internal_links(soup, url)\n",
        "        if url not in internal_links:\n",
        "            internal_links.insert(0, url)\n",
        "\n",
        "        found_on_pages = []\n",
        "        for page_url in internal_links[:10]:\n",
        "            try:\n",
        "                _, _, page_soup = scrape_dynamic(page_url)\n",
        "                results = search_keyword_in_soup(page_soup, keyword)\n",
        "            except RuntimeError:\n",
        "                results = search_keyword_on_page(page_url, keyword)\n",
        "            if results:\n",
        "                found_on_pages.append((page_url, results))\n",
        "\n",
        "        if not found_on_pages:\n",
        "            print(\"\\n Keyword not found on any scanned pages.\")\n",
        "        else:\n",
        "            print(f\"\\n Keyword found on {len(found_on_pages)} page(s):\\n\")\n",
        "            for page_url, snippets in found_on_pages:\n",
        "                print(f\"üîó Page: {page_url}\")\n",
        "                for _, snippet in snippets[:3]:\n",
        "                    print(f\"  üìù ...{snippet}...\\n\")\n",
        "\n",
        "    else:\n",
        "        extractor_map = {\n",
        "            'headlines': extract_headlines,\n",
        "            'title': lambda s: title,\n",
        "            'links': extract_links,\n",
        "            'images': extract_images,\n",
        "            'paragraphs': extract_paragraphs,\n",
        "            'text': lambda s: text\n",
        "        }\n",
        "        extractor = extractor_map.get(target_info)\n",
        "        extracted = extractor(soup) if extractor else \"No data extracted.\"\n",
        "\n",
        "        print(f\"\\n Title: {title}\")\n",
        "        print(f\"\\n Extracted Content Preview:\\n{extracted[:1500]}...\\n\")\n",
        "        print(f\" Requested Data: {target_info}\")"
      ],
      "metadata": {
        "id": "oQDCcdqR5XkA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwKWOQ0rtKO_",
        "outputId": "d151f194-5961-4772-aa63-a09c765a79ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Enter the website URL to scrape: https://wiki.openstreetmap.org/\n",
            "üéØ What do you want to extract? Choose one:\n",
            "    1: headlines\n",
            "    2: title\n",
            "    3: text (full page text)\n",
            "    4: links\n",
            "    5: images\n",
            "    6: content previews\n",
            "    7: search keyword in paragraphs\n",
            "Your choice (1-7): 6\n",
            "\n",
            " Attempting dynamic scrape with Firecrawl API...\n",
            " Firecrawl dynamic scrape failed: Firecrawl scraping failed: 402 Client Error: Payment Required for url: https://api.firecrawl.dev/v1/scrape\n",
            " Falling back to static scrape...\n",
            "\n",
            " Title: OpenStreetMap Wiki\n",
            "\n",
            " Extracted Content Preview:\n",
            "More about OpenStreetMap|How to contribute|Where to get help...\n",
            "\n",
            " Requested Data: paragraphs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itZpPpafn9Ei",
        "outputId": "8cdc4be9-cf13-4076-cbfd-1e528dab6b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Enter the website URL to scrape: https://wiki.openstreetmap.org/\n",
            "üéØ What do you want to extract? Choose one:\n",
            "    1: headlines\n",
            "    2: title\n",
            "    3: text (full page text)\n",
            "    4: links\n",
            "    5: images\n",
            "    6: paragraphs\n",
            "    7: search keyword in paragraphs\n",
            "Your choice (1-7): 5\n",
            "\n",
            " Attempting dynamic scrape with Firecrawl API...\n",
            " Firecrawl dynamic scrape failed: Firecrawl scraping failed: 402 Client Error: Payment Required for url: https://api.firecrawl.dev/v1/scrape\n",
            " Falling back to static scrape...\n",
            "\n",
            " Title: OpenStreetMap Wiki\n",
            "\n",
            " Extracted Content Preview:\n",
            "üñºÔ∏è /cc-wiki.png\n",
            "üñºÔ∏è https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Nuvola_web_pen.svg/80px-Nuvola_web_pen.svg.png\n",
            "üñºÔ∏è https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Preferences-system.svg/80px-Preferences-system.svg.png\n",
            "üñºÔ∏è /w/resources/assets/poweredby_mediawiki.svg\n",
            "üñºÔ∏è https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Openstreetmap_logo.svg/100px-Openstreetmap_logo.svg.png\n",
            "üñºÔ∏è /w/extensions/OSMCALWikiWidget/resources/osmcal-icon.png\n",
            "üñºÔ∏è https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\n",
            "üñºÔ∏è https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Openstreetmap_logo.svg/80px-Openstreetmap_logo.svg.png...\n",
            "\n",
            " Requested Data: images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioet6C9H59To",
        "outputId": "bbdf48b8-0933-4cc8-98de-73e9358b54f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Enter the website URL to scrape: https://www.bbc.com/\n",
            "üéØ What do you want to extract? Choose one:\n",
            "    1: headlines\n",
            "    2: title\n",
            "    3: text (full page text)\n",
            "    4: links\n",
            "    5: images\n",
            "    6: content previews\n",
            "    7: search keyword in paragraphs\n",
            "Your choice (1-7): 7\n",
            "\n",
            " Attempting dynamic scrape with Firecrawl API...\n",
            " Firecrawl dynamic scrape failed: Firecrawl scraping failed: 429 Client Error: Too Many Requests for url: https://api.firecrawl.dev/v1/scrape\n",
            " Falling back to static scrape...\n",
            " Enter the keyword to search for in paragraphs: culture\n",
            "\n",
            " Searching keyword 'culture' across multiple pages (limit 10 pages)...\n",
            "\n",
            "\n",
            " Keyword found on 3 page(s):\n",
            "\n",
            "üîó Page: https://www.bbc.com/travel/destinations/middle-east\n",
            "  üìù ...helping to preserve traditional Bedouin culture....\n",
            "\n",
            "  üìù ...As social distancing lingers, many cultures around the world are adapting their distinct greetings to fit the new normal....\n",
            "\n",
            "üîó Page: https://www.bbc.com/travel/destinations/europe\n",
            "  üìù ...neighbourhoods, an explosion of art and culture is transforming a once-neglected stretch into one of the city's hottest destinations....\n",
            "\n",
            "  üìù ...neighbourhoods, an explosion of art and culture is transforming a once-neglected stretch into one of the city's hottest destinations....\n",
            "\n",
            "  üìù ...o its fascinating history and authentic culture....\n",
            "\n",
            "üîó Page: https://www.bbc.com/travel/destinations/africa\n",
            "  üìù ...by sharing folk traditions, rituals and cultures through food....\n",
            "\n",
            "  üìù ...by sharing folk traditions, rituals and cultures through food....\n",
            "\n"
          ]
        }
      ]
    }
  ]
}